{"id":35,"date":"2023-01-17T08:11:50","date_gmt":"2023-01-17T08:11:50","guid":{"rendered":"http:\/\/localhost\/wordpress\/?p=35"},"modified":"2023-01-17T08:11:50","modified_gmt":"2023-01-17T08:11:50","slug":"ai-working-system","status":"publish","type":"post","link":"http:\/\/localhost\/wordpress\/ai-working-system\/","title":{"rendered":"Ai Working System"},"content":{"rendered":"\n<p>Enabling this early exit strategy for LMs requires minimal modifications to the training and inference processes. During training, we encourage the model to produce meaningful representations in intermediate layers. Instead of predicting only using the top layer, our learning loss function is a weighted average over the predictions of all layers, assigning higher weight to top layers. Our experiments demonstrate that this significantly improves the intermediate layer predictions while preserving the full model\u2019s performance. In one model variant, we also include a small&nbsp;<em><a href=\"https:\/\/arxiv.org\/abs\/2104.08803\">early-exit classifier<\/a><\/em>&nbsp;trained to classify if the local intermediate layer prediction is consistent with the top layer. We train this classifier in a second quick step where we freeze the rest of the model.<\/p>\n\n\n\n<p>Once the model is trained, we need a method to allow early-exiting. First, we define a local confidence measure for capturing the model\u2019s confidence in its intermediate prediction. We explore three confidence measures (described in the results section below): (1)&nbsp;<em><a href=\"https:\/\/arxiv.org\/abs\/2207.07061\">softmax response<\/a><\/em>, taking the maximum predicted probability out of the softmax distribution; (2)&nbsp;<em><a href=\"https:\/\/arxiv.org\/abs\/2207.07061\">state propagation<\/a><\/em>, the cosine distance between the current hidden representation and the one from the previous layer; and (3)&nbsp;<em>early-exit classifier,&nbsp;<\/em>the output of a classifier specifically trained for predicting local consistency. We find the softmax response to be statistically strong while being simple and fast to compute. The other two alternatives are lighter in&nbsp;<a href=\"https:\/\/en.wikipedia.org\/wiki\/Floating-point_arithmetic\">floating point operations<\/a>&nbsp;(FLOPS).<\/p>\n\n\n\n<p>Another challenge is that the&nbsp;<a href=\"https:\/\/en.wikipedia.org\/wiki\/Transformer_(machine_learning_model)#Self-Attention\">self-attention<\/a>&nbsp;of each layer depends on hidden-states from previous words. If we exit early for some word predictions, these hidden-states might be missing. Instead, we attend back to the hidden state of the last computed layer.<\/p>\n\n\n\n<p>Finally, we set up the local confidence threshold for exiting early. In the next section, we describe our controlled process for finding good threshold values. As a first step, we simplify this infinite search space by building on a useful observation: mistakes that are made at the beginning of the generation process are more detrimental since they can affect all of the following outputs. Therefore, we start with a higher (more conservative) threshold, and gradually reduce it with time. We use a negative exponent with user-defined temperature to control this decay rate. We find this allows better control over the performance-efficiency tradeoff (the obtained speedup per quality level).<\/p>\n\n\n\n<p>We run experiments on three popular generation datasets:&nbsp;<a href=\"https:\/\/github.com\/abisee\/cnn-dailymail\">CNN\/DM<\/a>&nbsp;for summarization,&nbsp;<a href=\"https:\/\/www.statmt.org\/wmt15\/translation-task.html\">WMT<\/a>&nbsp;for machine translation, and&nbsp;<a href=\"https:\/\/rajpurkar.github.io\/SQuAD-explorer\/\">SQuAD<\/a>&nbsp;for question answering. We evaluate each of the three confidence measures (softmax response, state propagation and early-exit classifier) using an 8-layer encoder-decoder model. To evaluate global sequence-level performance, we use the standard&nbsp;<a href=\"https:\/\/en.wikipedia.org\/wiki\/ROUGE_(metric)\">Rouge-L<\/a>,&nbsp;<a href=\"https:\/\/en.wikipedia.org\/wiki\/BLEU\">BLEU<\/a>, and&nbsp;<a href=\"https:\/\/arxiv.org\/abs\/1606.05250\">Token-F1<\/a>&nbsp;scores that measure distances against human-written references. We show that one can maintain full model performance while using only a third or half of the layers on average. CALM achieves this by dynamically distributing the compute effort across the prediction timesteps.<\/p>\n\n\n\n<p>As an approximate upper bound, we also compute the predictions using a&nbsp;<a href=\"https:\/\/arxiv.org\/abs\/2207.07061\">local oracle confidence measure<\/a>, which enables exiting at the first layer that leads to the same prediction as the top one. On all three tasks, the oracle measure can preserve full model performance when using only 1.5 decoder layers on average. In contrast to CALM, a static baseline uses the same number of layers for all predictions, requiring 3 to 7 layers (depending on the dataset) to preserve its performance. This demonstrates why the dynamic allocation of compute effort is important. Only a small fraction of the predictions require most of the model\u2019s complexity, while for others much less should suffice.<\/p>\n","protected":false},"excerpt":{"rendered":"<p>Enabling this early exit strategy for LMs requires minimal modifications to the training and inference processes. During training, we encourage the model to produce meaningful representations in intermediate layers. Instead of predicting only using the top layer, our learning loss function is a weighted average over the predictions of all layers, assigning higher weight to&hellip; <a class=\"more-link\" href=\"http:\/\/localhost\/wordpress\/ai-working-system\/\">Continue reading <span class=\"screen-reader-text\">Ai Working System<\/span><\/a><\/p>\n","protected":false},"author":1,"featured_media":36,"comment_status":"open","ping_status":"open","sticky":false,"template":"","format":"standard","meta":[],"categories":[1],"tags":[],"_links":{"self":[{"href":"http:\/\/localhost\/wordpress\/wp-json\/wp\/v2\/posts\/35"}],"collection":[{"href":"http:\/\/localhost\/wordpress\/wp-json\/wp\/v2\/posts"}],"about":[{"href":"http:\/\/localhost\/wordpress\/wp-json\/wp\/v2\/types\/post"}],"author":[{"embeddable":true,"href":"http:\/\/localhost\/wordpress\/wp-json\/wp\/v2\/users\/1"}],"replies":[{"embeddable":true,"href":"http:\/\/localhost\/wordpress\/wp-json\/wp\/v2\/comments?post=35"}],"version-history":[{"count":1,"href":"http:\/\/localhost\/wordpress\/wp-json\/wp\/v2\/posts\/35\/revisions"}],"predecessor-version":[{"id":37,"href":"http:\/\/localhost\/wordpress\/wp-json\/wp\/v2\/posts\/35\/revisions\/37"}],"wp:featuredmedia":[{"embeddable":true,"href":"http:\/\/localhost\/wordpress\/wp-json\/wp\/v2\/media\/36"}],"wp:attachment":[{"href":"http:\/\/localhost\/wordpress\/wp-json\/wp\/v2\/media?parent=35"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"http:\/\/localhost\/wordpress\/wp-json\/wp\/v2\/categories?post=35"},{"taxonomy":"post_tag","embeddable":true,"href":"http:\/\/localhost\/wordpress\/wp-json\/wp\/v2\/tags?post=35"}],"curies":[{"name":"wp","href":"https:\/\/api.w.org\/{rel}","templated":true}]}}